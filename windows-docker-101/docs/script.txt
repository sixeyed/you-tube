
Windows Containers and Docker: 101

Hey, how you doing? I'm Elton and this is a 101 for Windows Containers and Docker. If you've heard the news that Docker runs natively on Windows 10 and Windows Server 2016, but all you know about Docker is they have a cool logo, then this is the walkthrough for you. I'll take 15 minutes to show you what Docker is, how to use it with Windows, and *why* it's such an important technology. And it's a great technology, Docker can improve pretty much every aspect of delivering and running software.

First up here's the background. Docker is an application packaging technology - you use it to create a single package which bundles up your entire application. So you can Dockerize an ASP.NET website, and what you do is build one package which has your web application with all its dependencies, plus the platform, .NET with all its dependencies, plus the host - so that's IIS, plus the operating system - and it's all in one unit.

Docker calls that unit an image, and when you've packaged your ASP.NET app as an image you can run it on any Windows machine that has Docker installed. You run the image in a container, which is an isolated environment on the machine. The Windows box you run the container on doesn't need IIS installed or .NET or anything except Docker. Everything that your app needs - including all its configuration - that's all baked into the image, so wherever you run it, it will behave in the same way.

We'll see how that looks for an ASP.NET app shortly, but we'll start with something simple. 

* demo

You build a Docker image by creating a text file called a Dockerfile, where you specify all the instructions to set up your app. This is a command-line world, containers don't have a UI, so if your PowerShell isn't too hot, it soon will be. The Dockerfile is the source code for your image, but the syntax is really simple - you only need to know half a dozen instructions to build production-grade images.

This image just has three instructions - FROM is mandatory, it says which image we're going to use as the base for this image. This is like saying which operating system you want to start with, I'm using Microsoft Nano Server, and this line will give me a clean install of Nano Server to use as the basis for my app.

Next I'm going to copy a file from my local machine into the Docker image - so this is how you would copy your compiled application, or your scripts, or whatever else your app needs to run. In this case it's just one file which is a simple PowerShell script that prints out environment variables.  The last instruction is the command - this tells Docker what to do when you run the image, so here we're just going to execute the PowerShell script.

The Dockerfile is the source and I build it into an image with the docker build command. Every image needs a tag, which is effectively the image name, and I need to tell Docker where to find the Dockerfile and the content for the image - that's any resources you use in the Dockerfile, like my PowerShell script - I can just use dot if it's all in the current directory.

The first time you run this, Docker will see that you don't have the Microsoft Nano Server image and it will download it from the Docker Hub - which is a public, shared store for images. I already have the base image locally, so Docker just runs through each instruction in the Dockerfile and the final output is a new image with the name I gave it.

The docker images command lists all the images on your machine, and here's the new one I've just built. The image is just the application package remember - there's nothing running yet, all I've done is bundle up my application.

To run it I use the docker run command, giving the name of the image, and what Docker will do is create a container for my app from the image - that's an isolated sandbox where the applcation runs. Here's the output from my script and we can see a bunch of environment variables - the operating system is Windows NT and the hostname starts with B9B. 

If I repeat that command, I'll get a new hostname written out - D53 - because Docker creates a new container for every run. Inside the container the app thinks it's running in a separate machine, with its own host name and IP address. 

Okay, so we ran some containers that just did one thing and wrote out some content - so what happened to them when the PowerShell command finished? docker ps lists all the running containers, and there aren't any. Those containers ran and exited - I can see them in the exited status when I run `docker ps --all`. Docker monitors the process it starts when it runs a container, which was the PowerShell script in this case. When the process ends, then the container exits.

* slides

We've seen how to build a Dockerfile into a simple image and run a container from it. My image just copies one PowerShell script on top of the Nano Server base image, and configures Docker to run the script when it starts a container.

This is a task container which just does one thing and exits, and it's a really useful pattern. My container executes a trivial script, but it could do anything. I could build an image which has scripts to create a whole cloud infrastructure - so when you run the container it spins up an Azure Resource group, virtual networks, storage accounts, VMs, everything you need. 

The smart thing is, in that image you'd have all the PowerShell modules you need, as well as  your scripts, and you can pass variables to the container when you run it, so you've got a complete package for creating a new environment. Anyone can use it - you don't need the right version of PowerShell, or the AzureRM modules installed - you just need Docker.

Task containers are great for automation becuase the image has everything it needs, there are no additional steps or hidden dependencies. Docker's a command line tool with a REST API behind it, so it integrates easily with other tools like build servers and even the Windows Task Scheduler - you could run containers to set up and tear down environments, back up databases, send out mailshots - anything you want to automate.

There are two other ways to run containers. Let's look at interactive and background containers.


* demo

My `print-env` image is built to do one thing, but I can use it in different ways. Instead of just running it as a task container, I can use the interactive options to connect to the container, and I can pass a different command to run when it starts. This is the same image running as an interactive container, and I'm connected to a PowerShell session inside the container.

The container is from the same image, so my environment script is here - but it didn't execute when the container started because I told Docker to run an interactive PowerShell session instead. 

I can use an interactive container just like a remote machine or a VM. I can run ping and we see the container is connected to the outside world, ipconfig shows me the container has its own IP address, and hostname shows me the random ID that Docker generated for this container.

In here I can do anything that the OS lets me do - this is Nano Server, so I can't install MSIs but I can check on windows services, and I could install features like DNS. Interactive containers are great for navigating around to see what an image can do, or if you want to test out the steps for a Dockerfile when you build your own image. 

When I exit powershell, that's the end of the process which Docker started, so that container exits and there's nothing here in the docker ps output. The terminal colours get a bit mixed up, but that's a minor thing.

The last way to run a container is the one you'll use most - the background container. Again I can use the same image, there's nothing special about images that mean they have to be used in certain ways. This time I'll use the detached option, so Docker will put this container in the background.

So now when I run docker ps - ah, it's not there. Which is right - even though it's a background container, when the process inside it finishes the container still exits. This container will have run the environment script and then exited. If I want to keep a container running in the background, I need to keep the process inside it running.

So we'll do this instead - the same command to run a detached container, but when it starts it will ping docker.com 100 times. When the container starts I get control back in the terminal, and docker ps shows me the container is running in the background. I won't cover all the docker commands here, but docker logs is a good one - it shows me what's being logged by a container, in this case that's all my ping results.

This isn't an endless process lke a web server though, so when we reach the hundredth ping, this container will exit too.


* slides

We've seen how to build an image, and how to run it in different ways - as a task container which does one thing and stops; as an interactve container whch you connect to like a remote machine; and a background container which keeps an app running, like a Windows Service. They're all useful for different scenarios, and how they run depends on how you start the container - the same image can be used in different ways.

I'm going to focus on background containers now, because that's where a lot of the value comes from using Docker. Let's say you're running some websites and some REST APIs built with .NET - you could package up each of those apps into its own image and run them as Docker containers. But if you do that, what benefit is it going to give you?

When you start getting into Docker and running your apps as containers you'll find it's a technology that removes some of the big problems we have in the software industry. It makes your delivery process faster and more reliable, so you can release more often and get new features and fixes out more quickly. 

Docker can change your whole approach - the deployment for your app becomes defined in the Dockerfile which is part of your source code - that brings the operations side closer to the dev side and can help you move towards DevOps. 

Docker images can run on any platform with a matching OS, so if you're looking at cloud but don't want to commit to one vendor, Docker gives you platform independence. You can run your Docker container on AWS or Azure or in your data center or on your laptop - and it will be the exact same application no matter where you run it. 

And Docker has a whole set of features that support container orchestration, so you can run distributed systems where each part is in a container, and Docker takes care of connecting them - and that makes it easy to start breaking down monolithic apps into microservices.

Even if those trends are way down the line for you, there's practical value in using Docker right now. One of the biggest benefits is increased density. You can run a lot of containers on one machine, and they're all isolated from each other. Containers don't need allocated resources, the processes inside each container run as if they were running on the host. You don't give a share of CPU and memory to each container - they just use what they need (although you can put maximum limts on what each container can have). 

You can easily run dozens of apps in containers on a modest machine. They could be ASP.NET applications, or .NET Core, Go, Node, Python - whatever platform is best for solving the problem. They can all happily run on the same host, because they're isolated from each other, but they can communicate through Docker's networking layer. 

If someone needs a new environment - for testing a specific scenario, or to recreate a bug, you don't need to commission a new server and wait 6 weeks to get it; you don't even need to request a new VM and wait 6 hours for it to be provisioned and configured - you just run a container from the exact image you need, on your existing machine, and it's ready in seconds.

As long as your containers aren't all hitting peak load at the same time, you can run a lot of apps on one host. When you eventually do run out of CPU or RAM, you can join several servers running Docker into a swarm, but that's for another session.

We're nearly done, but we'll finish up with some real apps in action.


* demo

This Dockerfile is for a very simple ASP.NET Core web app, which calculates Pi. We have a familiar set of instructions - I'm starting from Microsoft's .NET Core base image, running on Windows Nano Server. The EXPOSE instruction is like a simple firewall rule for the image - it's saying the host can integrate with the container on port 5000. Then I set the working directory, abd copy in the compiled .NET Core app from my machine into the image.  And then the CMD instruction just starts the asp.net core application, listening on port 5000 on any IP address.

I've already built this image and shared it on the public Hub. I'll start the web app in the background with docker run -d and publish port 5000 in the container to the same port on the host. That means when the host gets a request on port 5000, it will forward it to the container. I'll give the container a name, so I can refer to it later, and the image is called sixeyed/pi-web-app:nanoserver; this will start in a couple of seconds.

`docker ps` tells me it's running, and I can browse to localhost port 5000 and see my awesome Pi app. Except I don't. There's a known limitation with the networking stack in windows which means if I'm logged into the host machine, I can't access the port that's mapped from the container. `docker inspect` tells me the IP address of the container, so if I browse to that then I see my awesome Pi app. If this box gets an exteral request on port 5000 then it will route it to the container, but on the host I can't use localhost - so be aware! It's only really a limitation when you're developing locally, but hopefully it will disappear with a future update.

Anyway, this is Pi to 6 decimal places, and I can change that with a query string to see Pi to 1,000 DP - the calcs are all running in .NET Core inside my Windows container. If I want to run another copy of the same app, I can do that - I need to publish to a different port, because hosts can't share ports, but I can publish this on port 5001. I need to get the container's IP address again to see it from the host, but I browse to that - it's the same app on the same host but its running in a completey independent container, which is a great way to run multiple non-production environments on one box.

Even better, and this is the big finale: I can run existing full ASP.NET apps as Docker containers too. I have a blog series where I'm dockerizing Nerd Dinner, which is an old ASP.NET MVC showcase app. It was last changed on CodePlex in 2013. The Dockerfile for this is a bit more involved, because the app runs in IIS and uses SQL Server LocalDB - so there's a bunch of configuration, but it's configuration we only do once to build the image, and ultimately this is a tiny amount of work to be able to package up a three year old app and run it in Docker.

The docker run command is the same, I'll publish port 8081 which the image exposes, and the image is called sixeyed/nerd-dinner:part1. This image is also on the Hub so you can run this yourself - it's a 4GB download though, so it will take a while, unless you already have the microsoft/iis base image locally. And it takes a little longer to start because the IIS windows service starts when the container runs.

But here it is, and I can now browse to port 8001 and I see the NerdDinner app. This is a legacy ASP.NET app with no code changes, running on Docker. On my machine now I have two ASP.NET Core app running in Nano Server containers, and a full ASP.NET app running in a Windows Server Core container. They're isolated and I don't have any limits so they can both use as much CPU and RAM as they need.


* slides

Okay, that's it for the 101, let's recap. 

Docker is a technology for packaging a whole application into a single image, which has everything the app needs. You run the app in a container, which is an isolated environment created from the image. You can Dockerize new apps and old apps, and you can run as many containers on your machine as it can handle - containers don't use much compute resource unless they're doing something.

With Windows 10 and Windows Server 2016 you can package Windows apps and run them using Docker natively. At the moment you can only run Windows containers on Windows and Linux containers on Linux - so be aware of that. But if you want to mix and match applications from different platforms you can join Windows and Linux machines into one Docker Swarm and use it as a cross-platform compute cluster.

We had a quick look at the Dockerfile, which is the source code for packaging an application, saw how to build that into an image, and how to run containers in different ways to achieve different goals. Docker has been massively popular in the Linux world, because it's a great technology that's easy to get started with and it helps remove a lot of techical problems so you can focus on business problems and deliver more value, more quickly.

What next? If you're on Twitter, you can follow the Docker Captains - that's Docker's recognition program, like Microsoft have MVPs. The captains are always blogging and speak regularly at events around the world, and they're a great source of information. The documentation on Docker's website and on MSDN is excellent, and there are a bunch of sample Windows Container images on GitHub that you cna use for inspiration.

You should follow me on Twitter, obviously, and check out my blog or my GitHub repo to see what I'm doing with my Dockerized Nerd Dinner project - there's a roadmap here for taking it from a monolithic ASP.NET app to a distributed system where the most valuable parts ar in small, easily changed microservices.

And finally - one of the reason's Docker is so popular is because it has a great technical community. Check out MeetUp - there's almost certainly a Docker user group near you. They'll have regular sessions with great talks; the ones I've been to have been super friendly and very welcoming for newcomers. If you're n the UK I'll probably see you some time at Docker London. Until then I'll be putting more videos like this up on YouTube, to help you on your journey with Windows and Docker.




